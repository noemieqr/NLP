{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, LSTM, RepeatVector, Dense, Activation, Input, Flatten, Reshape, Permute, Lambda\n",
    "from keras.layers.merge import multiply, concatenate\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "#import seq2seq\n",
    "#from seq2seq.models import AttentionSeq2Seq\n",
    "\n",
    "import data_loader\n",
    "import postprocessing\n",
    "\n",
    "\n",
    "def main():\n",
    "    path_to_data_dir = 'data/'\n",
    "    path_to_embeddings_dir = 'embeddings/'\n",
    "\n",
    "    use_pretrained_embeddings = True        # set to True to use a pre-trained word embedding model\n",
    "    split_mrs = True                        # set to True to split the test MRs before predicting\n",
    "    postprocess = True                      # set to False to skip the utterance post-processing\n",
    "    max_input_seq_len = 30                  # number of words the MRs should be truncated/padded to\n",
    "    max_output_seq_len = 50                 # number of words the utterances should be truncated/padded to\n",
    "    vocab_size = 10000                      # maximum vocabulary size of the utterances\n",
    "    num_variations = 3                      # number of MR permutations to consider for re-ranking\n",
    "    depth_enc = 1                           # number of LSTM layers in the encoder\n",
    "    depth_dec = 1                           # number of LSTM layers in the decoder\n",
    "    hidden_layer_size = 500                 # number of neurons in a single LSTM layer\n",
    "\n",
    "\n",
    "    # ---- WORD EMBEDDING ----\n",
    "    print('\\nLoading embedding model...')\n",
    "    embedding_model = data_loader.load_embedding_model(path_to_data_dir, path_to_embeddings_dir, use_pretrained_embeddings)\n",
    "    weights = embedding_model.syn0\n",
    "\n",
    "    # DEBUG PRINT\n",
    "    print('weights.shape =', weights.shape)\n",
    "    #print(embedding_model.similarity('pizza', 'hamburger'))\n",
    "    #print(embedding_model.similarity('pizza', 'furniture'))\n",
    "    #print(embedding_model.similarity('coffee', 'tea'))\n",
    "\n",
    "\n",
    "    # ---- LOAD DATA ----\n",
    "    print('\\nLoading data...')\n",
    "    #word2idx, idx2word = data_loader.load_vocab(path_to_vocab)\n",
    "    x_train, y_train, x_test, y_test, original_mrs, original_sents, test_groups, y_idx2word = \\\n",
    "            data_loader.load_data(path_to_data_dir, embedding_model, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, split_mrs)\n",
    "\n",
    "    # x_test, y_test = permute_input(original_mrs, original_sents)\n",
    "\n",
    "    # DEBUG PRINT\n",
    "    print('Utterance vocab size:', len(y_idx2word))\n",
    "    print('x_train.shape =', x_train.shape)\n",
    "    print('y_train.shape =', y_train.shape)\n",
    "    print('x_test.shape =', x_test.shape)\n",
    "    print('y_test.shape =', y_test.shape)\n",
    "\n",
    "\n",
    "    # ---- BUILD THE MODEL ----\n",
    "    print('\\nBuilding language generation model...')\n",
    "    #model = Sequential()\n",
    "\n",
    "    #ret_seq_first_layer = False\n",
    "    #if depth_enc > 1:\n",
    "    #    ret_seq_first_layer = True\n",
    "\n",
    "    ## -- ENCODER --\n",
    "    ##model.add(Embedding(input_dim=weights.shape[0],\n",
    "    ##                    output_dim=weights.shape[1],\n",
    "    ##                    weights=[weights],\n",
    "    ##                    input_length=max_seq_len,       # can be omitted to process sequences of heterogenous length\n",
    "    ##                    trainable=False))\n",
    "    #model.add(Bidirectional(LSTM(units=weights.shape[1],\n",
    "    #                             dropout=0.2,\n",
    "    #                             recurrent_dropout=0.2,\n",
    "    #                             return_sequences=ret_seq_first_layer),\n",
    "    #                        input_shape=(max_input_seq_len, weights.shape[1])))\n",
    "    #if depth_enc > 2:\n",
    "    #    for d in range(depth_enc - 2):\n",
    "    #        model.add(Bidirectional(LSTM(units=weights.shape[1],\n",
    "    #                                        dropout=0.2,\n",
    "    #                                        recurrent_dropout=0.2,\n",
    "    #                                     return_sequences=True)))\n",
    "    #if depth_enc > 1:\n",
    "    #    model.add(Bidirectional(LSTM(units=weights.shape[1],\n",
    "    #                                    dropout=0.2,\n",
    "    #                                    recurrent_dropout=0.2,\n",
    "    #                                 return_sequences=False)))\n",
    "\n",
    "    ## -- DECODER --\n",
    "    #model.add(RepeatVector(max_output_seq_len))\n",
    "    #for d in range(depth_dec):\n",
    "    #    model.add(LSTM(units=weights.shape[1],\n",
    "    #                   dropout=0.2,\n",
    "    #                   recurrent_dropout=0.2,\n",
    "    #                   return_sequences=True))\n",
    "    #model.add(TimeDistributed(Dense(len(y_idx2word),\n",
    "    #                                activation='softmax')))\n",
    "\n",
    "\n",
    "    # ---- ATTENTION MODEL ----\n",
    "\n",
    "    input = Input(shape=(max_input_seq_len, weights.shape[1]))\n",
    "\n",
    "    # -- ENCODER --\n",
    "    encoder = Bidirectional(LSTM(units=hidden_layer_size,\n",
    "                                 dropout=0.2,\n",
    "                                 recurrent_dropout=0.2,\n",
    "                                 return_sequences=True),\n",
    "                            merge_mode='concat')(input)\n",
    "\n",
    "    # -- ATTENTION --\n",
    "    flattened = Flatten()(encoder)\n",
    "\n",
    "    attention = []\n",
    "    for i in range(max_output_seq_len):\n",
    "        weighted = Dense(max_input_seq_len, activation='softmax')(flattened)\n",
    "        unfolded = Permute([2, 1])(RepeatVector(hidden_layer_size * 2)(weighted))\n",
    "        multiplied = multiply([encoder, unfolded])\n",
    "        summed = Lambda(lambda x: K.sum(x, axis=-2))(multiplied)\n",
    "        attention.append(Reshape((1, hidden_layer_size * 2))(summed))\n",
    "\n",
    "    attention_out = concatenate(attention, axis=-2)\n",
    "\n",
    "    # -- DECODER --\n",
    "    decoder = LSTM(units=hidden_layer_size,\n",
    "                   dropout=0.2,\n",
    "                   recurrent_dropout=0.2,\n",
    "                   return_sequences=True)(attention_out)\n",
    "\n",
    "    decoder = Dense(len(y_idx2word),\n",
    "                    activation='softmax')(decoder)\n",
    "\n",
    "    model = Model(inputs=input, outputs=decoder)\n",
    "\n",
    "\n",
    "    # ---- Keras Seq2Seq attention model [https://github.com/farizrahman4u/seq2seq] (not working) ----\n",
    "    #model = AttentionSeq2Seq(input_dim=weights.shape[1],\n",
    "    #                         input_length=max_input_seq_len,\n",
    "    #                         hidden_dim=hidden_layer_size,\n",
    "    #                         output_length=max_output_seq_len,\n",
    "    #                         output_dim=len(y_idx2word),\n",
    "    #                         depth=1)\n",
    "\n",
    "\n",
    "    # ---- COMPILE ----\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # -- Define Checkpoint--\n",
    "    #filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    filepath = 'trained_model.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    \n",
    "    # ---- TRAIN ----\n",
    "    print('\\nTraining...')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=20,\n",
    "              callbacks=callbacks_list)\n",
    "    \n",
    "    \n",
    "    # ---- TEST ----\n",
    "    #print('\\nTesting...')\n",
    "    #score, acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "    #print()\n",
    "    #print('-> Test score:', score)\n",
    "    #print('-> Test accuracy:', acc)\n",
    "\n",
    "\n",
    "    # ---- PREDICT ----\n",
    "    print('\\nPredicting...')\n",
    "\n",
    "    # -- SINGLE PREDICTION --\n",
    "    #prediction_distr = model.predict(np.array([x_test[123]]))       # test MR: name[The Rice Boat], food[Japanese], area[city centre]\n",
    "    #prediction = np.argmax(prediction_distr, axis=2)                # note: prediction_distr is a 3D array even for a single input to model.predict()\n",
    "    #utterance = [y_idx2word[idx] for idx in prediction[0] if idx > 0]\n",
    "    #print(' '.join(utterance))\n",
    "\n",
    "    # -- BATCH PREDICTION --\n",
    "    results = []\n",
    "    prediction_distr = model.predict(np.array(x_test))\n",
    "    predictions = np.argmax(prediction_distr, axis=2)\n",
    "\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        utterance = ' '.join([y_idx2word[idx] for idx in prediction if idx > 0])\n",
    "        results.append(utterance)\n",
    "\n",
    "    # print(len(original_mrs))\n",
    "    # print(len(results))\n",
    "    print(\"Predictions have been processed. Now we are depermuting them: \")\n",
    "    # x, y, p = postprocessing.depermute_input(original_mrs, original_sents, results, num_variations)\n",
    "    # correct_preds = postprocessing.correct(x, p)\n",
    "    # print(len(original_mrs))\n",
    "    # print(len(results))\n",
    "    if split_mrs:\n",
    "        results_merged = postprocessing.merge_utterances(results, original_mrs, test_groups, num_variations)\n",
    "    else:\n",
    "        results_merged = []\n",
    "        for i, prediction in enumerate(results):\n",
    "            results_merged.append(postprocessing.relex_utterance(prediction, original_mrs[i]))\n",
    "\n",
    "    #todo add this\n",
    "    # if not split_mrs:\n",
    "    #     utterance = postprocessing.relex_utterance(utterance, original_mrs[i])\n",
    "\n",
    "    np.savetxt('results/results_raw.txt', list(results_merged), fmt='%s')\n",
    "    # print('\\n'.join(results_merged))\n",
    "\n",
    "\n",
    "    # ---- POST-PROCESS ----\n",
    "    if postprocess:\n",
    "        print(\"Predictions have been processed. Now we are depermuting them: \")\n",
    "        x, y, p = postprocessing.depermute_input(original_mrs, original_sents, results_merged, num_variations)\n",
    "        print(\"Depermution is done, files written.\")\n",
    "        print(\"Writing depermute file.\")\n",
    "        cp = postprocessing.combo_print(p, results_merged, num_variations)\n",
    "        correct_preds = postprocessing.correct(x, p)\n",
    "\n",
    "        # for pp in p:\n",
    "        #     print(pp)\n",
    "        np.savetxt('results/results_pooling.txt', list(p), fmt='%s')\n",
    "        np.savetxt('results/results_combo_pool.txt', list(cp), fmt='%s')\n",
    "        np.savetxt('results/results_pooling_corrected.txt', list(correct_preds), fmt='%s')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(int(main() or 0))\n",
    "\n",
    "    # t = \"The Golden Currey serves Fast food food near near\"\n",
    "    # y = \"The Golden Currey is rated a 3 3 of a of 5 5 5 5\"\n",
    "    # x = \"The Golden Currey is near near the city centre\"\n",
    "    # blah = [t, y, x]\n",
    "    # mrs = [0,0,0]\n",
    "    # # t = \"The Golden Currey is a family near near\"\n",
    "    # # t = score_grammar_spelling(t, True)\n",
    "    # tool = language_check.LanguageTool('en-US')\n",
    "    # for g in blah:\n",
    "    #     print(score_grammar_spelling(False, g, tool))\n",
    "    #     print(score_known_errors(g))\n",
    "    # print(correct(mrs, blah))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
