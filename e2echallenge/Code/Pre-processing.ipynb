{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def create_embeddings(file_paths, path_to_embeddings, path_to_vocab, path_to_model, **params):\n",
    "    class SentenceGenerator(object):\n",
    "        def __init__(self, file_paths):\n",
    "            self.file_paths = file_paths\n",
    "\n",
    "        def __iter__(self):\n",
    "            for file_path in self.file_paths:\n",
    "                for line in open(file_path):\n",
    "                    # tokenize\n",
    "                    yield simple_preprocess(line)\n",
    "\n",
    "    sentences = SentenceGenerator(file_paths)\n",
    "\n",
    "    model = Word2Vec(sentences, **params)\n",
    "    model.wv.save_word2vec_format(path_to_model)\n",
    "    weights = model.wv.syn0\n",
    "    np.save(open(path_to_embeddings, 'wb'), weights)\n",
    "\n",
    "    vocab = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "    with open(path_to_vocab, 'w') as f:\n",
    "        f.write(json.dumps(vocab))\n",
    "\n",
    "# TODO: rewrite into object-oriented\n",
    "\n",
    "def load_embedding_model(path_to_data_dir, path_to_embeddings_dir, use_pretrained_embeddings):\n",
    "    path_to_training = path_to_data_dir + 'trainset_perm_3_slot_mr.csv'\n",
    "    path_to_test = path_to_data_dir + 'devset_3_slot_mr.csv'\n",
    "    path_to_embeddings = path_to_embeddings_dir + 'embeddings.npy'\n",
    "    path_to_vocab = path_to_embeddings_dir + 'vocab.json'\n",
    "    path_to_model = path_to_embeddings_dir + 'embedding_model.bin'\n",
    "    #path_to_pretrained_model = path_to_embeddings_dir + 'GoogleNews-vectors-negative300.bin'\n",
    "    path_to_pretrained_model = path_to_embeddings_dir + 'glove.6B.300d.txt'\n",
    "\n",
    "\n",
    "    if use_pretrained_embeddings:\n",
    "        # load Google's word2vec pre-trained word embedding model\n",
    "        #return KeyedVectors.load_word2vec_format(path_to_pretrained_model, binary=True)\n",
    "\n",
    "        # load Stanford's GloVe pre-trained word embedding model\n",
    "        return KeyedVectors.load_word2vec_format(path_to_pretrained_model, binary=False)\n",
    "    else:\n",
    "        # train custom embedding model, if necessary\n",
    "        if (os.path.isdir(path_to_embeddings_dir) == False or os.path.isfile(path_to_embeddings) == False):\n",
    "            embedding.create_embeddings([path_to_training, path_to_test],\n",
    "                                        path_to_embeddings,\n",
    "                                        path_to_vocab,\n",
    "                                        path_to_model,\n",
    "                                        size=100,\n",
    "                                        min_count=2,\n",
    "                                        window=5,\n",
    "                                        iter=1)\n",
    "\n",
    "        # load our trained word2vec model\n",
    "        return KeyedVectors.load_word2vec_format(path_to_model, binary=False)\n",
    "\n",
    "\n",
    "def load_data(path_to_data_dir, embedding_model, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, split_mrs):\n",
    "    path_to_training = path_to_data_dir + 'trainset.csv'\n",
    "    #path_to_training = path_to_data_dir + 'trainset_perm_3_slot_mr.csv'\n",
    "    path_to_test = path_to_data_dir + 'devset.csv'\n",
    "    #path_to_test = path_to_data_dir + 'devset_3_slot_mr.csv'\n",
    "    #path_to_data_embed = path_to_data_dir + 'data_embed.pkl'\n",
    "    \n",
    "\n",
    "    # store/load the data in the embedded form\n",
    "    #if os.path.isfile(path_to_data_embed) == False:\n",
    "    #    x_train, y_train, x_test, y_test = preprocess_data(path_to_training, path_to_test, embedding_model, max_seq_len)\n",
    "    #    with open(path_to_data_embed, 'wb') as f:\n",
    "    #        pickle.dump([x_train, y_train, x_test, y_test], f)\n",
    "    #else:\n",
    "    #    with open(path_to_data_embed, 'rb') as f:\n",
    "    #        x_train, y_train, x_test, y_test = pickle.load(f)\n",
    "\n",
    "    #return x_train, y_train, x_test, y_test\n",
    "\n",
    "    return preprocess_data(path_to_training, path_to_test, embedding_model, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, split_mrs)\n",
    "\n",
    "\n",
    "def preprocess_data(path_to_training_data, path_to_test_data, embedding, vocab_size, max_input_seq_len, max_output_seq_len, num_variations, use_split_mrs):\n",
    "    # read the training data from file\n",
    "    data_frame_train = pd.read_csv(path_to_training_data, header=0, encoding='latin1')  # names=['mr', 'ref']\n",
    "    x_train = data_frame_train.mr.tolist()\n",
    "    y_train = data_frame_train.ref.tolist()\n",
    "\n",
    "    # read the test data from file\n",
    "    data_frame_test = pd.read_csv(path_to_test_data, header=0, encoding='latin1')       # names=['mr', 'ref']\n",
    "    x_test = data_frame_test.mr.tolist()\n",
    "    y_test = data_frame_test.ref.tolist()\n",
    "\n",
    "    original_mrs = copy.deepcopy(x_test)\n",
    "    original_sents = copy.deepcopy(y_test)\n",
    "\n",
    "    if use_split_mrs:\n",
    "        # split MRs into shorter ones\n",
    "        x_test, y_test, test_groups = split_mrs(x_test, y_test, num_variations=num_variations)\n",
    "    elif num_variations > 1:\n",
    "        x_test, y_test = permute_input(x_test, y_test, num_permutes=num_variations)\n",
    "        test_groups = []\n",
    "    else:\n",
    "        test_groups = []\n",
    "\n",
    "\n",
    "    # parse the utterances into lists of words\n",
    "    y_train = [preprocess_utterance(y) for y in y_train]\n",
    "    y_test = [preprocess_utterance(y) for y in y_test]\n",
    "\n",
    "    # create utterance vocabulary\n",
    "    distr = FreqDist(np.concatenate(y_train + y_test))\n",
    "    y_vocab = distr.most_common(min(len(distr), vocab_size))        # cap the vocabulary size\n",
    "    y_idx2word = [word[0] for word in y_vocab]\n",
    "    y_idx2word.insert(0, '-PADDING-')\n",
    "    y_idx2word.extend(['&slot_val_name&', '&slot_val_food&', '&slot_val_near&'])\n",
    "    y_idx2word.append('-PERIOD-')\n",
    "    y_idx2word.append('-NA-')\n",
    "    y_word2idx = {word: idx for idx, word in enumerate(y_idx2word)}\n",
    "\n",
    "    delex_data(x_train, y_train, update_data_source=True)\n",
    "    delex_data(x_test, y_test, update_data_source=True)\n",
    "    \n",
    "\n",
    "    padding_vec = np.zeros(embedding.syn0.shape[1])         # embedding vector for \"padding\" words\n",
    "\n",
    "    # produce sequences of embedding vectors from the meaning representations (MRs) in the training set\n",
    "    x_train_seq = []\n",
    "    for mr in x_train:\n",
    "        row_list = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot and convert to embedding\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            row_list.extend([embedding[slot_word] for slot_word in slot.split() if slot_word in embedding.vocab])\n",
    "            # parse the value and convert to embedding\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            row_list.extend([embedding[value_word] for value_word in value.split() if value_word in embedding.vocab])\n",
    "        # add padding\n",
    "        row_list = add_padding(row_list, padding_vec, max_input_seq_len)\n",
    "\n",
    "        x_train_seq.append(row_list)\n",
    "\n",
    "    # produce sequences of one-hot vectors from the reference utterances in the training set\n",
    "    y_train_seq = np.zeros((len(y_train), max_output_seq_len, len(y_word2idx)), dtype=np.int8)\n",
    "    for i, utterance in enumerate(y_train):\n",
    "        for j, word in enumerate(utterance):\n",
    "            # truncate long utterances\n",
    "            if j >= max_output_seq_len:\n",
    "                break\n",
    "\n",
    "            # represent each word with a one-hot vector\n",
    "            if word == '.':\n",
    "                y_train_seq[i][j][y_word2idx['-PERIOD-']] = 1\n",
    "            elif word in y_word2idx:\n",
    "                y_train_seq[i][j][y_word2idx[word]] = 1\n",
    "            else:\n",
    "                y_train_seq[i][j][y_word2idx['-NA-']] = 1\n",
    "\n",
    "        # add padding for short utterances\n",
    "        for j in range(len(utterance), max_output_seq_len):\n",
    "            y_train_seq[i][j][y_word2idx['-PADDING-']] = 1\n",
    "\n",
    "    # produce sequences of embedding vectors from the meaning representations (MRs) in the test set\n",
    "    x_test_seq = []\n",
    "    for mr in x_test:\n",
    "        row_list = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot and convert to embedding\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            row_list.extend([embedding[slot_word] for slot_word in slot.split() if slot_word in embedding.vocab])\n",
    "            # parse the value and convert to embedding\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            row_list.extend([embedding[value_word] for value_word in value.split() if value_word in embedding.vocab])\n",
    "        # add padding\n",
    "        row_list = add_padding(row_list, padding_vec, max_input_seq_len)\n",
    "\n",
    "        x_test_seq.append(row_list)\n",
    "\n",
    "    # produce sequences of one-hot vectors from the reference utterances in the test set\n",
    "    y_test_seq = np.zeros((len(y_test), max_output_seq_len, len(y_word2idx)), dtype=np.int8)\n",
    "    for i, utterance in enumerate(y_test):\n",
    "        for j, word in enumerate(utterance):\n",
    "            # truncate long utterances\n",
    "            if j >= max_output_seq_len:\n",
    "                break\n",
    "\n",
    "            # represent each word with a one-hot vector\n",
    "            if word in y_word2idx:\n",
    "                y_test_seq[i][j][y_word2idx[word]] = 1\n",
    "            else:\n",
    "                y_test_seq[i][j][y_word2idx['-NA-']] = 1\n",
    "\n",
    "        # add padding for short utterances\n",
    "        for j in range(len(utterance), max_output_seq_len):\n",
    "            y_test_seq[i][j][y_word2idx['-PADDING-']] = 1\n",
    "\n",
    "    return (np.array(x_train_seq), np.array(y_train_seq), np.array(x_test_seq), np.array(y_test_seq), original_mrs, original_sents, test_groups, y_idx2word)\n",
    "\n",
    "\n",
    "def permute_input(mrs, sents, num_permutes):\n",
    "    new_mr = []\n",
    "    new_sent = []\n",
    "    for x, mr in enumerate(mrs):\n",
    "        sentence = sents[x]\n",
    "        temp = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            temp.append(slot + '[' + value + ']')\n",
    "        for t in range(0, num_permutes):\n",
    "            temptemp = copy.deepcopy(temp)\n",
    "            random.shuffle(temptemp)\n",
    "            curr_mr = ', '.join(temptemp)\n",
    "            new_mr.append(curr_mr)\n",
    "            new_sent.append(sentence)\n",
    "    return new_mr, new_sent\n",
    "\n",
    "\n",
    "def split_mrs(mrs, utterances, num_variations):\n",
    "    new_mrs = []\n",
    "    new_utterances = []\n",
    "    groups = []\n",
    "    group_id = 0\n",
    "\n",
    "    for idx, mr in enumerate(mrs):\n",
    "        utterance = utterances[idx]\n",
    "        # do not split short MRs\n",
    "        if len(mr) < 4:\n",
    "            new_mrs.append(mr)\n",
    "            new_utterances.append(utterance)\n",
    "            continue\n",
    "\n",
    "        slot_value_list = []\n",
    "        name_slot = ()\n",
    "\n",
    "        # parse the slot-value pairs\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "\n",
    "            if slot == 'name':\n",
    "                name_slot = (slot, value)\n",
    "            else:\n",
    "                slot_value_list.append((slot, value))\n",
    "\n",
    "        for i in range(num_variations):\n",
    "            slot_value_list_copy = slot_value_list[:]\n",
    "            random.shuffle(slot_value_list_copy)\n",
    "\n",
    "            # distribute the slot-value pairs as multiple shorter MRs\n",
    "            while len(slot_value_list_copy) > 0:\n",
    "                # include the name slot by default in each subset\n",
    "                mr_subset = [name_slot]\n",
    "                # add up to two other slots to the subset\n",
    "                for i in range(min(2, len(slot_value_list_copy))):\n",
    "                    mr_subset.append(slot_value_list_copy.pop())\n",
    "            \n",
    "                new_mr = [s + '[' + v + ']' for s, v in mr_subset]\n",
    "                new_mrs.append(', '.join(new_mr))\n",
    "                new_utterances.append(utterance)\n",
    "                groups.append(group_id)\n",
    "            \n",
    "            group_id += 1\n",
    "\n",
    "    return new_mrs, new_utterances, groups\n",
    "\n",
    "\n",
    "def preprocess_utterance(utterance, keep_periods=False):\n",
    "    if keep_periods:\n",
    "        chars_to_filter = '!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    \n",
    "        # add spaces before periods so they can be parsed as individual words\n",
    "        utterance = utterance.replace('. ', ' . ')\n",
    "        if utterance[-1] == '.':\n",
    "            utterance = utterance[:-1] + ' ' + utterance[-1]\n",
    "\n",
    "        return text_to_word_sequence(utterance, filters=chars_to_filter)\n",
    "    else:\n",
    "        chars_to_filter = '.!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "        return text_to_word_sequence(utterance, filters=chars_to_filter)\n",
    "\n",
    "\n",
    "def delex_data(mrs, sentences, update_data_source=False, specific_slots=None, split=True):\n",
    "    if specific_slots is not None:\n",
    "        delex_slots = specific_slots\n",
    "    else:\n",
    "        delex_slots = ['name', 'food', 'near']\n",
    "\n",
    "    for x, mr in enumerate(mrs):\n",
    "        if split:\n",
    "            sentence = ' '.join(sentences[x])\n",
    "        else:\n",
    "            sentence = sentences[x].lower()\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            if slot in delex_slots:\n",
    "                value = slot_value[sep_idx + 1:-1].strip()\n",
    "                sentence = sentence.replace(value.lower(), '&slot_val_{0}&'.format(slot))\n",
    "                mr = mr.replace(value, '&slot_val_{0}&'.format(slot))\n",
    "                # if not split:\n",
    "                #     print(\"delex:\")\n",
    "                #     print('&slot_val_{0}&'.format(slot))\n",
    "                #     print(value.lower())\n",
    "                #     print(sentence)\n",
    "        if update_data_source:\n",
    "            if split:\n",
    "                sentences[x] = sentence.split()\n",
    "            else:\n",
    "                sentences[x] = sentence\n",
    "            mrs[x] = mr\n",
    "        if not split:\n",
    "            return sentence\n",
    "        # new_sent = relex_sentences(mr, sentence)\n",
    "\n",
    "\n",
    "def add_padding(seq, padding_vec, max_seq_len):\n",
    "    diff = max_seq_len - len(seq)\n",
    "    if diff > 0:\n",
    "        # pad short sequences\n",
    "        return seq + [padding_vec for i in range(diff)]\n",
    "    else:\n",
    "        # truncate long sequences\n",
    "        return seq[:max_seq_len]\n",
    "\n",
    "\n",
    "def load_vocab(path_to_vocab):\n",
    "    with open(path_to_vocab, 'r') as f_vocab:\n",
    "        data = json.loads(f_vocab.read())\n",
    "\n",
    "    word2idx = data\n",
    "    idx2word = {v: k for k, v in data.items()}\n",
    "\n",
    "    return word2idx, idx2word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
