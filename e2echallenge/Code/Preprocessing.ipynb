{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import FreqDist\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    train = pd.read_csv(path+'/trainset.csv')\n",
    "    dev = pd.read_csv(path+'/devset.csv')\n",
    "    test = pd.read_csv(path+'/testset.csv')\n",
    "    test_ref = pd.read_csv(path+'/testset_w_refs.csv')\n",
    "    return (train, dev, test, test_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42061, 2)\n",
      "(4672, 2)\n",
      "(4693, 2)\n",
      "(630, 1)\n"
     ]
    }
   ],
   "source": [
    "train, dev, test, test_ref = load_data('../e2e-dataset')\n",
    "print(train.shape)\n",
    "print(dev.shape)\n",
    "print(test_ref.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = True        # set to True to use a pre-trained word embedding model\n",
    "use_split_mrs = False                        # set to True to split the test MRs before predicting\n",
    "postprocess = True                      # set to False to skip the utterance post-processing\n",
    "max_input_seq_len = 30                  # number of words the MRs should be truncated/padded to\n",
    "max_output_seq_len = 50                 # number of words the utterances should be truncated/padded to\n",
    "vocab_size = 10000                      # maximum vocabulary size of the utterances\n",
    "num_variations = 3                      # number of MR permutations to consider for re-ranking\n",
    "depth_enc = 1                           # number of LSTM layers in the encoder\n",
    "depth_dec = 1                           # number of LSTM layers in the decoder\n",
    "hidden_layer_size = 500                 # number of neurons in a single LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(file_paths, **params):\n",
    "    class SentenceGenerator(object):\n",
    "        def __init__(self, file_paths):\n",
    "            self.file_paths = file_paths\n",
    "\n",
    "        def __iter__(self):\n",
    "            for file_path in self.file_paths:\n",
    "                for line in open(file_path):\n",
    "                    # tokenize\n",
    "                    yield simple_preprocess(line)\n",
    "\n",
    "    sentences = SentenceGenerator(file_paths)\n",
    "\n",
    "    model = Word2Vec(sentences, **params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_input(mrs, sents, num_permutes):\n",
    "   \n",
    "    new_mr = []\n",
    "    new_sent = []\n",
    "    for x, mr in enumerate(mrs):\n",
    "        sentence = sents[x]\n",
    "        temp = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            temp.append(slot + '[' + value + ']')\n",
    "        \n",
    "       # num_permutes= math.factorial(len(temp))\n",
    "        for t in range(0, num_permutes):\n",
    "            temptemp = copy.deepcopy(temp)\n",
    "            random.shuffle(temptemp)\n",
    "            curr_mr = ', '.join(temptemp)\n",
    "            new_mr.append(curr_mr)\n",
    "            new_sent.append(sentence)\n",
    "            \n",
    "    return new_mr, new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mrs(mrs, utterances, num_variations):\n",
    "\n",
    "    new_mrs = []\n",
    "    new_utterances = []\n",
    "    groups = []\n",
    "    group_id = 0\n",
    "\n",
    "    for idx, mr in enumerate(mrs):\n",
    "        utterance = utterances[idx]\n",
    "        # do not split short MRs\n",
    "        if len(mr) < 4:\n",
    "            new_mrs.append(mr)\n",
    "            new_utterances.append(utterance)\n",
    "            continue\n",
    "\n",
    "        slot_value_list = []\n",
    "        name_slot = ()\n",
    "\n",
    "        # parse the slot-value pairs\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "\n",
    "            if slot == 'name':\n",
    "                name_slot = (slot, value)\n",
    "            else:\n",
    "                slot_value_list.append((slot, value))\n",
    "\n",
    "        for i in range(num_variations):\n",
    "            slot_value_list_copy = slot_value_list[:]\n",
    "            random.shuffle(slot_value_list_copy)\n",
    "\n",
    "            # distribute the slot-value pairs as multiple shorter MRs\n",
    "            while len(slot_value_list_copy) > 0:\n",
    "                # include the name slot by default in each subset\n",
    "                mr_subset = [name_slot]\n",
    "                # add up to two other slots to the subset\n",
    "                for i in range(min(2, len(slot_value_list_copy))):\n",
    "                    mr_subset.append(slot_value_list_copy.pop())\n",
    "            \n",
    "                new_mr = [s + '[' + v + ']' for s, v in mr_subset]\n",
    "                new_mrs.append(', '.join(new_mr))\n",
    "                new_utterances.append(utterance)\n",
    "                groups.append(group_id)\n",
    "            \n",
    "            group_id += 1\n",
    "\n",
    "    return new_mrs, new_utterances, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_utterance(utterance):\n",
    "    chars_to_filter = '.!\"#$%&()*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    return text_to_word_sequence(utterance, filters=chars_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delex_data(mrs, sentences, update_data_source=False,  split=True):\n",
    "\n",
    "    delex_slots = ['name', 'food', 'near']\n",
    "\n",
    "    for x, mr in enumerate(mrs):\n",
    "       \n",
    "        sentence = ' '.join(sentences[x])\n",
    "    \n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            if slot in delex_slots:\n",
    "                value = slot_value[sep_idx + 1:-1].strip()\n",
    "                name = '&slot_val_{0}&'+slot\n",
    "                sentence = sentence.replace(value.lower(), name)\n",
    "                mr = mr.replace(value, name)\n",
    "\n",
    "        sentences[x] = sentence.split()\n",
    "        mrs[x] = mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(seq, padding_vec, max_seq_len):\n",
    "    diff = max_seq_len - len(seq)\n",
    "    if diff > 0:\n",
    "        # pad short sequences\n",
    "        return seq + [padding_vec for i in range(diff)]\n",
    "    else:\n",
    "        # truncate long sequences\n",
    "        return seq[:max_seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce sequences of embedding vectors from the meaning representations\n",
    "def seq_emb (data , embedding, vocab, max_input_seq_len , padding_vec):  \n",
    "    data_seq=[]\n",
    "    for mr in data:\n",
    "        row_list = []\n",
    "        for slot_value in mr.split(','):\n",
    "            sep_idx = slot_value.find('[')\n",
    "            # parse the slot and convert to embedding\n",
    "            slot = slot_value[:sep_idx].strip()\n",
    "            row_list.extend([embedding[slot_word] for slot_word in slot.split() if slot_word in vocab])\n",
    "            # parse the value and convert to embedding\n",
    "            value = slot_value[sep_idx + 1:-1].strip()\n",
    "            row_list.extend([embedding[value_word] for value_word in value.split() if value_word in vocab])\n",
    "        # add padding\n",
    "        row_list = add_padding(row_list, padding_vec, max_input_seq_len)\n",
    "        data_seq.append(row_list)\n",
    "    \n",
    "    return data_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " # produce sequences of one-hot vectors from the reference utterances\n",
    "def seq_one_hot(y, y_word2idx, max_output_seq_len):\n",
    "\n",
    "    y_seq = np.zeros((len(y), max_output_seq_len, len(y_word2idx)), dtype=np.int8)\n",
    "    for i, utterance in enumerate(y):\n",
    "        for j, word in enumerate(utterance):\n",
    "            # truncate long utterances\n",
    "            if j >= max_output_seq_len:\n",
    "                break\n",
    "\n",
    "            # represent each word with a one-hot vector\n",
    "            if word == '.':\n",
    "                y_seq[i][j][y_word2idx['-PERIOD-']] = 1\n",
    "            elif word in y_word2idx:\n",
    "                y_seq[i][j][y_word2idx[word]] = 1\n",
    "            else:\n",
    "                y_seq[i][j][y_word2idx['-NA-']] = 1\n",
    "\n",
    "        # add padding for short utterances\n",
    "        for j in range(len(utterance), max_output_seq_len):\n",
    "            y_seq[i][j][y_word2idx['-PADDING-']] = 1\n",
    "\n",
    "    return y_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train, dev, test_ref , vocab_size, max_input_seq_len, max_output_seq_len, num_variations, use_split_mrs):\n",
    "    \n",
    "    x_train = train.mr.tolist()\n",
    "    y_train = train.ref.tolist()\n",
    "    \n",
    "    x_dev = dev.mr.tolist()\n",
    "    y_dev = dev.ref.tolist()\n",
    "    \n",
    "    x_test_ref = test_ref.mr.tolist()\n",
    "    y_test_ref = test_ref.ref.tolist()\n",
    "    \n",
    "    \n",
    "    original_mrs_dev = copy.deepcopy(x_dev)\n",
    "    original_sents_dev = copy.deepcopy(y_dev)\n",
    "    \n",
    "    original_mrs_test = copy.deepcopy(x_test_ref)\n",
    "    original_sents_test = copy.deepcopy(y_test_ref)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dev_groups = []\n",
    "    test_groups = []\n",
    "    \n",
    "    if use_split_mrs:\n",
    "        # split MRs into shorter ones\n",
    "        x_dev, y_dev, dev_groups = split_mrs(x_dev, y_dev, num_variations=num_variations)\n",
    "        x_test_ref, y_test_ref, test_groups = split_mrs(x_test_ref, y_test_ref, num_variations=num_variations)\n",
    "   \n",
    "    elif num_variations > 1:\n",
    "        x_dev, y_dev = permute_input(x_dev, y_dev, num_permutes=num_variations)\n",
    "        x_test_ref, y_test_ref = permute_input(x_test_ref, y_test_ref, num_permutes=num_variations)\n",
    "\n",
    "        \n",
    "        \n",
    "    # parse the utterances into lists of words\n",
    "    y_train = [preprocess_utterance(y) for y in y_train]\n",
    "    y_dev = [preprocess_utterance(y) for y in y_dev]\n",
    "    y_test_ref = [preprocess_utterance(y) for y in y_test_ref]\n",
    "\n",
    "    \n",
    "    \n",
    "    # create utterance vocabulary\n",
    "    distr = FreqDist(np.concatenate(y_train + y_dev))\n",
    "    y_vocab = distr.most_common(min(len(distr), vocab_size))        # cap the vocabulary size\n",
    "    y_idx2word = [word[0] for word in y_vocab]\n",
    "    y_idx2word.insert(0, '-PADDING-')\n",
    "    y_idx2word.extend(['&slot_val_name&', '&slot_val_food&', '&slot_val_near&'])\n",
    "    y_idx2word.append('-PERIOD-')\n",
    "    y_idx2word.append('-NA-')\n",
    "    y_word2idx = {word: idx for idx, word in enumerate(y_idx2word)}\n",
    "\n",
    "    \n",
    "    #Delexicalization\n",
    "    delex_data(x_train, y_train, update_data_source=True)\n",
    "    delex_data(x_dev, y_dev, update_data_source=True)\n",
    "    \n",
    "    \n",
    "    #Embeddings\n",
    "    path='../e2e-dataset'\n",
    "    path_to_training = path+'/trainset.csv'\n",
    "    path_to_test = path+'/devset.csv'\n",
    "    \n",
    "    embedding = create_embeddings([path_to_training, path_to_test],size=100,min_count=2,window=5,iter=1)\n",
    "    \n",
    "    weights = embedding.wv.syn0\n",
    "    vocab = dict([(k, v.index) for k, v in embedding.wv.vocab.items()])\n",
    "\n",
    "    \n",
    "    padding_vec = np.zeros(weights.shape[1])         # embedding vector for \"padding\" words\n",
    "    \n",
    "    # produce sequences of embedding vectors from the meaning representations (MRs) in the training /dev/test set\n",
    "    x_train_seq =  seq_emb (x_train , embedding, vocab, max_input_seq_len , padding_vec)\n",
    "    x_dev_seq =  seq_emb (x_dev , embedding, vocab, max_input_seq_len , padding_vec)\n",
    "    x_test_seq =  seq_emb (x_test_ref , embedding, vocab, max_input_seq_len , padding_vec)\n",
    "\n",
    "    \n",
    "    # produce sequences of one-hot vectors from the reference utterances in the training /dev/test set\n",
    "    y_train_seq= seq_one_hot(y_train, y_word2idx, max_output_seq_len)\n",
    "    y_dev_seq= seq_one_hot(y_dev, y_word2idx, max_output_seq_len)\n",
    "    y_test_seq= seq_one_hot(y_test_ref, y_word2idx, max_output_seq_len)\n",
    "\n",
    "\n",
    "    result = [np.array(x_train_seq), np.array(y_train_seq), np.array(x_dev_seq), np.array(y_dev_seq), np.array(x_test_seq), np.array(y_test_seq), original_mrs_dev, original_mrs_test, original_sents_dev, original_sents_test, test_groups, dev_groups, y_idx2word] \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_split_mrs=True\n",
    "res = preprocess_data(train, dev, test_ref , vocab_size, max_input_seq_len, max_output_seq_len, num_variations, use_split_mrs)  \n",
    "#x_train_seq, y_train_seq , x_dev_seq , y_dev_seq , x_test_seq , y_test_seq ,original_mrs_dev , original_mrs_test,  test_groups, dev_groups, y_idx2word =preprocess_data(train, dev, test_ref , vocab_size, max_input_seq_len, max_output_seq_len, num_variations, use_split_mrs)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_split_mrs=False\n",
    "res = preprocess_data(train, dev, test_ref , vocab_size, max_input_seq_len, max_output_seq_len, num_variations, use_split_mrs)  \n",
    "#x_train_seq, y_train_seq , x_dev_seq , y_dev_seq , x_test_seq , y_test_seq ,original_mrs_dev , original_mrs_test,  test_groups, dev_groups, y_idx2word =preprocess_data(train, dev, test_ref , vocab_size, max_input_seq_len, max_output_seq_len, num_variations, use_split_mrs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- weights \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_input_seq_len, weights.shape[1]))\n",
    "# ---- ATTENTION MODEL ----\n",
    "\n",
    "    input = Input(shape=(max_input_seq_len, weights.shape[1]))\n",
    "\n",
    "    # -- ENCODER --\n",
    "    encoder = Bidirectional(LSTM(units=hidden_layer_size,\n",
    "                                 dropout=0.2,\n",
    "                                 recurrent_dropout=0.2,\n",
    "                                 return_sequences=True),\n",
    "                            merge_mode='concat')(input)\n",
    "\n",
    "    # -- ATTENTION --\n",
    "    flattened = Flatten()(encoder)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
